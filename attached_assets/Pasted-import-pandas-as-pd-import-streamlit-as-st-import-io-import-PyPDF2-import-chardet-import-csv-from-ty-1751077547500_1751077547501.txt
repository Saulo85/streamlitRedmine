import pandas as pd
import streamlit as st
import io
import PyPDF2
import chardet
import csv
from typing import Optional, List, Dict, Any, Tuple


def detect_encoding(file_content: bytes) -> Dict[str, Any]:
    """Detect encoding of file content using chardet."""
    try:
        detection_result = chardet.detect(file_content)
        encodings_to_try = [
            detection_result.get('encoding', 'utf-8'),
            'utf-8',
            'latin-1',
            'cp1252'
        ]
        return {
            'detected_encoding': detection_result.get('encoding', 'unknown'),
            'confidence': detection_result.get('confidence', 0.0),
            'encodings_to_try': list(dict.fromkeys(filter(None, encodings_to_try)))
        }
    except Exception as e:
        return {
            'detected_encoding': 'utf-8',
            'confidence': 0.0,
            'encodings_to_try': ['utf-8', 'latin-1', 'cp1252'],
            'error': str(e)
        }

def detect_csv_separator(file_content: str, sample_size: int = 1024) -> Dict[str, Any]:
    """Detect CSV separator by analyzing file content."""
    sample = file_content[:sample_size]
    separators = [',', ';', '\t', '|']
    separator_counts = {sep: sample.count(sep) for sep in separators}
    detected_separator = max(separator_counts, key=separator_counts.get) if separator_counts else ','

    return {
        'detected_separator': detected_separator,
        'separator_counts': separator_counts
    }

def try_read_csv_with_params(file_content: str, encoding: str, separator: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """Try to read CSV with specific encoding and separator."""
    try:
        string_io = io.StringIO(file_content)
        df = pd.read_csv(
            string_io,
            sep=separator,
            encoding=encoding,
            low_memory=False,
            na_values=['', 'NULL', 'null', 'N/A', 'n/a', 'NA', 'na']
        )
        metadata = {
            'success': True,
            'rows': len(df),
            'columns': len(df.columns),
            'column_names': list(df.columns)
        }
        return df, metadata
    except Exception as e:
        return pd.DataFrame(), {'success': False, 'error': str(e)}

def process_csv_file(csv_file) -> pd.DataFrame:
    """Process uploaded CSV file with robust encoding and separator detection."""
    try:
        csv_file.seek(0)
        raw_content = csv_file.read()
        st.info(f"ðŸ“„ Arquivo carregado: {csv_file.name} ({len(raw_content)} bytes)")
    except Exception as e:
        raise Exception(f"Erro ao ler arquivo: {str(e)}")

    try:
        encoding_info = detect_encoding(raw_content)
        st.success(f"ðŸ” CodificaÃ§Ã£o detectada: {encoding_info['detected_encoding']} (confianÃ§a: {encoding_info['confidence']:.1%})")
        encoding = encoding_info['encodings_to_try'][0]
    except Exception as e:
        st.warning(f"âš ï¸ Erro na detecÃ§Ã£o de codificaÃ§Ã£o: {str(e)}")
        encoding = 'utf-8'

    try:
        decoded_content = raw_content.decode(encoding)
    except Exception as e:
        raise Exception("NÃ£o foi possÃ­vel decodificar o arquivo com a codificaÃ§Ã£o testada.")

    separator_info = detect_csv_separator(decoded_content)
    st.success(f"ðŸ“Š Separador detectado: '{separator_info['detected_separator']}'")

    df, metadata = try_read_csv_with_params(decoded_content, encoding, separator_info['detected_separator'])

    if df.empty:
        st.error("âŒ NÃ£o foi possÃ­vel ler o arquivo CSV")
        return df

    # Mapeamento de colunas de acordo com o novo formato
    column_mapping = {
        '#': 'id',
        'TÃ­tulo': 'subject',
        'Cliente': 'client',
        'Sistema': 'system',
        'Criado em': 'created_date',
        'ConcluÃ­do': 'completed_date',
        'Autor': 'author',
        'DescriÃ§Ã£o': 'description',
        'Ãšltimas notas': 'notes'
    }

    df = df.rename(columns=column_mapping)

    # VerificaÃ§Ã£o de colunas obrigatÃ³rias
    required_columns = ['id', 'subject', 'client', 'system', 'created_date']
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        raise Exception(f"Colunas obrigatÃ³rias nÃ£o encontradas: {', '.join(missing_columns)}")

    df['upload_timestamp'] = pd.Timestamp.now()

    return df
   
        
    except pd.errors.EmptyDataError:
        raise Exception("Arquivo CSV estÃ¡ vazio")
    except pd.errors.ParserError as e:
        raise Exception(f"Erro ao analisar CSV: {str(e)}")
    except Exception as e:
        raise Exception(f"Erro no processamento do arquivo: {str(e)}")

def _clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Clean and standardize DataFrame data
    
    Args:
        df: Raw DataFrame
        
    Returns:
        Cleaned DataFrame
    """
    # Remove completely empty rows
    df = df.dropna(how='all')
    
    # Fill NaN values in text columns with empty strings
    text_columns = ['subject', 'description', 'assigned_to', 'client', 'status', 'priority', 'category']
    for col in text_columns:
        if col in df.columns:
            df[col] = df[col].fillna('')
    
    # Clean text fields
    for col in ['subject', 'description']:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip()
    
    # Ensure ID is string type for consistency
    if 'id' in df.columns:
        df['id'] = df['id'].astype(str)
    
    # Parse dates if available
    date_columns = ['created_date', 'updated_date', 'closed_date']
    for col in date_columns:
        if col in df.columns:
            try:
                df[col] = pd.to_datetime(df[col], errors='coerce')
            except:
                pass  # Keep original format if parsing fails
    
    return df

def process_pdf_file(pdf_file) -> Dict[str, Any]:
    """
    Process uploaded PDF file and extract text content
    
    Args:
        pdf_file: Uploaded PDF file from Streamlit
        
    Returns:
        Dictionary with PDF metadata and extracted text
        
    Raises:
        Exception: If PDF processing fails
    """
    try:
        # Read PDF content
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        # Extract text from all pages
        text_content = ""
        for page_num, page in enumerate(pdf_reader.pages):
            try:
                page_text = page.extract_text()
                text_content += f"\n--- Page {page_num + 1} ---\n{page_text}"
            except Exception as e:
                st.warning(f"Erro ao extrair texto da pÃ¡gina {page_num + 1}: {str(e)}")
        
        # Get PDF metadata
        metadata = {
            'filename': pdf_file.name,
            'num_pages': len(pdf_reader.pages),
            'text_content': text_content.strip(),
            'upload_timestamp': pd.Timestamp.now()
        }
        
        # Add PDF info if available
        if pdf_reader.metadata:
            metadata.update({
                'title': pdf_reader.metadata.get('/Title', ''),
                'author': pdf_reader.metadata.get('/Author', ''),
                'creator': pdf_reader.metadata.get('/Creator', ''),
                'creation_date': pdf_reader.metadata.get('/CreationDate', ''),
            })
        
        return metadata
        
    except Exception as e:
        raise Exception(f"Erro ao processar PDF: {str(e)}")

def validate_csv_structure(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Validate CSV structure and provide feedback
    
    Args:
        df: DataFrame to validate
        
    Returns:
        Dictionary with validation results
    """
    validation_result = {
        'is_valid': True,
        'warnings': [],
        'suggestions': [],
        'statistics': {}
    }
    
    # Check data quality
    total_rows = len(df)
    
    # Check for empty descriptions
    if 'description' in df.columns:
        empty_descriptions = df['description'].isna().sum() + (df['description'] == '').sum()
        if empty_descriptions > 0:
            validation_result['warnings'].append(
                f"{empty_descriptions} tickets tÃªm descriÃ§Ãµes vazias ({empty_descriptions/total_rows*100:.1f}%)"
            )
    
    # Check for duplicate tickets
    if 'id' in df.columns:
        duplicate_ids = df['id'].duplicated().sum()
        if duplicate_ids > 0:
            validation_result['warnings'].append(f"{duplicate_ids} IDs duplicados encontrados")
    
    # Statistics
    validation_result['statistics'] = {
        'total_tickets': total_rows,
        'columns_found': list(df.columns),
        'date_range': _get_date_range(df),
        'top_assignees': _get_top_values(df, 'assigned_to'),
        'top_clients': _get_top_values(df, 'client')
    }
    
    return validation_result

def _get_date_range(df: pd.DataFrame) -> Dict[str, str]:
    """Get date range from DataFrame"""
    date_info = {}
    
    for col in ['created_date', 'updated_date']:
        if col in df.columns:
            try:
                dates = pd.to_datetime(df[col], errors='coerce').dropna()
                if not dates.empty:
                    date_info[f'{col}_min'] = str(dates.min().date())
                    date_info[f'{col}_max'] = str(dates.max().date())
            except:
                pass
    
    return date_info

def _get_top_values(df: pd.DataFrame, column: str, top_n: int = 5) -> List[str]:
    """Get top N values from a column"""
    if column not in df.columns:
        return []
    
    try:
        top_values = df[column].value_counts().head(top_n)
        return [f"{value} ({count})" for value, count in top_values.items()]
    except:
        return []
